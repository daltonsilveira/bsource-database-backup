"""
M√≥dulo de abstra√ß√£o para providers de storage (S3-compatible).
Suporta Cloudflare R2 e AWS S3.
"""
import os
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Optional

import boto3
import pytz
from botocore.exceptions import ClientError, NoCredentialsError
import logging

log = logging.getLogger("backup.storage")


class StorageProvider(ABC):
    """Classe abstrata para providers de object storage."""

    def __init__(
        self,
        access_key_id: str,
        secret_access_key: str,
        bucket_name: str,
        destination_folder: Optional[str] = None,
    ):
        if not all([access_key_id, secret_access_key, bucket_name]):
            raise ValueError(
                "Configura√ß√µes de storage incompletas. "
                "Verifique STORAGE_ACCESS_KEY_ID, STORAGE_SECRET_ACCESS_KEY e STORAGE_BUCKET_NAME."
            )

        self.access_key_id = access_key_id
        self.secret_access_key = secret_access_key
        self.bucket_name = bucket_name
        self.destination_folder = destination_folder or "backups/"
        self._client = None

    @property
    def client(self):
        """Cliente S3 lazy-loaded ‚Äî implementado pelas subclasses."""
        if self._client is None:
            self._client = self._create_client()
        return self._client

    @abstractmethod
    def _create_client(self):
        """Cria e retorna o client boto3 configurado para o provider."""

    def upload_file(
        self,
        local_filepath: str,
        filename: str,
        metadata: Optional[dict] = None,
    ) -> str:
        """
        Faz upload do arquivo para o storage.

        Args:
            local_filepath: Caminho local do arquivo.
            filename: Nome do arquivo no bucket.
            metadata: Metadados adicionais para o arquivo.

        Returns:
            str: Caminho completo do arquivo no storage.

        Raises:
            FileNotFoundError: Arquivo local n√£o encontrado.
            ClientError: Erro na comunica√ß√£o com o storage.
            NoCredentialsError: Credenciais inv√°lidas ou ausentes.
        """
        if not os.path.exists(local_filepath):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {local_filepath}")

        destination_path = self._build_destination_path(filename)

        extra_args = {}
        if metadata:
            extra_args["Metadata"] = metadata

        log.info(f"üì§ Iniciando upload: {destination_path}")

        try:
            self.client.upload_file(
                local_filepath,
                self.bucket_name,
                destination_path,
                ExtraArgs=extra_args,
            )
            log.info("‚úÖ Upload conclu√≠do com sucesso")
            return destination_path

        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "Unknown")
            log.error(f"‚ùå Erro do cliente storage: {error_code}")
            raise
        except NoCredentialsError:
            log.error("‚ùå Credenciais de storage inv√°lidas ou ausentes")
            raise
        except Exception as e:
            log.error(f"‚ùå Erro inesperado no upload: {e}")
            raise

    def test_connection(self) -> bool:
        """Testa a conex√£o com o storage."""
        try:
            self.client.list_objects_v2(Bucket=self.bucket_name, MaxKeys=1)
            log.info("‚úÖ Conex√£o com storage testada com sucesso")
            return True
        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "Unknown")
            log.error(f"‚ùå Falha no teste de conex√£o: {error_code}")
            return False
        except Exception as e:
            log.error(f"‚ùå Erro inesperado no teste de conex√£o: {e}")
            return False

    def _build_destination_path(self, filename: str) -> str:
        """
        Constr√≥i o caminho de destino com organiza√ß√£o por data.

        Formato: destination_folder/YYYYMMDD/filename
        """
        base_folder = self.destination_folder.rstrip("/") + "/" if self.destination_folder else ""

        timezone_name = os.getenv("TIMEZONE", "America/Sao_Paulo")
        try:
            timezone = pytz.timezone(timezone_name)
        except pytz.UnknownTimeZoneError:
            timezone = pytz.timezone("America/Sao_Paulo")

        local_time = datetime.now(timezone)
        date_folder = local_time.strftime("%Y%m%d")

        return f"{base_folder}{date_folder}/{filename}"


# ‚îÄ‚îÄ Implementa√ß√µes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


class R2Storage(StorageProvider):
    """Storage provider para Cloudflare R2."""

    def __init__(
        self,
        endpoint_url: str,
        access_key_id: str,
        secret_access_key: str,
        bucket_name: str,
        destination_folder: Optional[str] = None,
    ):
        if not endpoint_url:
            raise ValueError(
                "STORAGE_ENDPOINT_URL √© obrigat√≥rio para Cloudflare R2."
            )
        self.endpoint_url = endpoint_url
        super().__init__(access_key_id, secret_access_key, bucket_name, destination_folder)

    def _create_client(self):
        return boto3.client(
            "s3",
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key_id,
            aws_secret_access_key=self.secret_access_key,
            region_name="auto",
        )


class S3Storage(StorageProvider):
    """Storage provider para AWS S3."""

    def __init__(
        self,
        access_key_id: str,
        secret_access_key: str,
        bucket_name: str,
        region: str,
        endpoint_url: Optional[str] = None,
        destination_folder: Optional[str] = None,
    ):
        if not region:
            raise ValueError(
                "STORAGE_REGION √© obrigat√≥rio para AWS S3 (ex: us-east-1)."
            )
        self.region = region
        self.endpoint_url = endpoint_url
        super().__init__(access_key_id, secret_access_key, bucket_name, destination_folder)

    def _create_client(self):
        kwargs = {
            "aws_access_key_id": self.access_key_id,
            "aws_secret_access_key": self.secret_access_key,
            "region_name": self.region,
        }
        if self.endpoint_url:
            kwargs["endpoint_url"] = self.endpoint_url

        return boto3.client("s3", **kwargs)


# ‚îÄ‚îÄ Factory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

SUPPORTED_STORAGE_TYPES = ["r2", "s3"]


def create_storage_from_env() -> StorageProvider:
    """
    Cria um StorageProvider a partir das vari√°veis de ambiente.

    Vari√°veis utilizadas:
        STORAGE_TYPE: Tipo de storage (r2, s3)
        STORAGE_ENDPOINT_URL: URL do endpoint (obrigat√≥rio para R2, opcional para S3)
        STORAGE_ACCESS_KEY_ID: Chave de acesso
        STORAGE_SECRET_ACCESS_KEY: Chave secreta
        STORAGE_BUCKET_NAME: Nome do bucket
        STORAGE_DESTINATION_FOLDER: Pasta de destino (padr√£o: backups/)
        STORAGE_REGION: Regi√£o AWS (obrigat√≥rio para S3)

    Returns:
        StorageProvider: Inst√¢ncia configurada do provider.

    Raises:
        ValueError: Se STORAGE_TYPE n√£o for suportado ou vari√°veis obrigat√≥rias faltarem.
    """
    storage_type = os.getenv("STORAGE_TYPE", "").lower().strip()

    if storage_type not in SUPPORTED_STORAGE_TYPES:
        raise ValueError(
            f"STORAGE_TYPE '{storage_type}' n√£o suportado. "
            f"Valores aceitos: {', '.join(SUPPORTED_STORAGE_TYPES)}"
        )

    access_key_id = os.getenv("STORAGE_ACCESS_KEY_ID")
    secret_access_key = os.getenv("STORAGE_SECRET_ACCESS_KEY")
    bucket_name = os.getenv("STORAGE_BUCKET_NAME")
    destination_folder = os.getenv("STORAGE_DESTINATION_FOLDER", "backups/")
    endpoint_url = os.getenv("STORAGE_ENDPOINT_URL")
    region = os.getenv("STORAGE_REGION")

    if storage_type == "r2":
        return R2Storage(
            endpoint_url=endpoint_url,
            access_key_id=access_key_id,
            secret_access_key=secret_access_key,
            bucket_name=bucket_name,
            destination_folder=destination_folder,
        )

    # storage_type == "s3"
    return S3Storage(
        access_key_id=access_key_id,
        secret_access_key=secret_access_key,
        bucket_name=bucket_name,
        region=region,
        endpoint_url=endpoint_url,
        destination_folder=destination_folder,
    )
